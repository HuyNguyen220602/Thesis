{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import heatmap\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import mne\n",
    "from mne.datasets import sample\n",
    "from mne.decoding import (\n",
    "    CSP,\n",
    "    GeneralizingEstimator,\n",
    "    LinearModel,\n",
    "    Scaler,\n",
    "    SlidingEstimator,\n",
    "    Vectorizer,\n",
    "    cross_val_multiscore,\n",
    "    get_coef,\n",
    ")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform, os, re, multiprocessing\n",
    "import pandas as pd\n",
    "import pyedflib\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import time  # Import the time module\n",
    "import os \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils import resample\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils import resample\n",
    "from scipy import stats\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/home/Duchuy220602/thesis/File_Feature_Pre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list = []\n",
    "# for filename in os.listdir(path):\n",
    "#     if filename.endswith('.csv'):\n",
    "#         list.append(os.path.join(path, filename))\n",
    "# print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summarize all files in the list\n",
    "# for file in list:\n",
    "#     print(f\"Summary of {file}:\")\n",
    "#     df = pd.read_csv(file)\n",
    "#     # print(df.info())\n",
    "#     # print(df.describe())\n",
    "#     # print(\"------------------------\")\n",
    "#     print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub1 = pd.read_csv(r'/home/Duchuy220602/thesis/File_Feature_Pre/brux.csv')\n",
    "# sub2 = pd.read_csv(r'/home/Duchuy220602/thesis/File_Feature_Pre/ins.csv')\n",
    "# sub3 = pd.read_csv(r'/home/Duchuy220602/thesis/File_Feature_Pre/n.csv')\n",
    "# sub4 = pd.read_csv(r'/home/Duchuy220602/thesis/File_Feature_Pre/narco.csv')\n",
    "# sub5 = pd.read_csv(r'/home/Duchuy220602/thesis/File_Feature_Pre/nfle.csv')\n",
    "# sub6 = pd.read_csv(r'/home/Duchuy220602/thesis/File_Feature_Pre/plm.csv')\n",
    "# sub7 = pd.read_csv(r'/home/Duchuy220602/thesis/File_Feature_Pre/rbd.csv')\n",
    "# sub8 = pd.read_csv(r'/home/Duchuy220602/thesis/File_Feature_Pre/sdb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make sure column names match\n",
    "# sub1.columns = sub2.columns = sub3.columns = sub4.columns = sub5.columns = sub6.columns = sub7.columns = sub8.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Concatenate the dataframes\n",
    "# concatenated_df = pd.concat([sub1, sub2, sub3, sub4, sub5, sub6, sub7, sub8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_df.to_csv(r'/home/Duchuy220602/thesis/File_Feature_Pre/File_total.csv', index=False, encoding='utf-8-sig', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the CSV file into a DataFrame\n",
    "# df = pd.read_csv(r'/home/Duchuy220602/thesis/File_Feature_Pre/File_total.csv')\n",
    "\n",
    "# # Group the DataFrame by class\n",
    "# grouped = df.groupby('Target')\n",
    "\n",
    "# # Find the size of the smallest class\n",
    "# min_size = grouped.size().min()\n",
    "\n",
    "# # Sample rows from each class to match the size of the smallest class\n",
    "# sampled_dfs = []\n",
    "# for _, group in grouped:\n",
    "#     sampled_dfs.append(group.sample(min_size))\n",
    "\n",
    "# # Concatenate the sampled rows for all classes into a new DataFrame\n",
    "# scaled_df = pd.concat(sampled_dfs)\n",
    "\n",
    "# # Reset index of the new DataFrame\n",
    "# scaled_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Now scaled_df contains an equal number of rows for each class\n",
    "# print(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the scaled DataFrame to a CSV file\n",
    "# scaled_df.to_csv('/home/Duchuy220602/thesis/File_Feature_Pre/Scaled_File_total.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(r'/home/Duchuy220602/thesis/File_Feature_Pre/Scaled_File_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "X = df.iloc[:-1].drop(columns=\"Target\")\n",
    "y = df.iloc[:-1][\"Target\"]\n",
    "\n",
    "smote = SMOTE(sampling_strategy = 'all',random_state=42)\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "X, y = smote.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Tạo một LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Chuyển đổi y thành dạng số\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=60)  # Keep 95% of the variance\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_test.shape[0] / X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is the given DataFrame\n",
    "target_counts = df['Target'].value_counts()\n",
    "total_rows = len(df)\n",
    "target_percentages = target_counts / total_rows * 100\n",
    "\n",
    "# Create a pie chart\n",
    "plt.pie(target_percentages, labels=target_counts.index, autopct='%1.1f%%')\n",
    "plt.axis('equal')\n",
    "plt.title('Percentage of Each Target Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is the given DataFrame\n",
    "target_counts = df['Target'].value_counts()\n",
    "total_rows = len(df)\n",
    "target_percentages = target_counts / total_rows * 100\n",
    "\n",
    "# Create labels with both percentage and count\n",
    "labels = [f'{target_counts.index[i]}: {target_counts[i]} ({target_percentages[i]:.1f}%)' for i in range(len(target_counts))]\n",
    "\n",
    "# Create a pie chart\n",
    "plt.pie(target_percentages, labels=labels, autopct='', startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.title('Percentage of Each Target Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use different params\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_pca, y_train)\n",
    "\n",
    "print(f\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test_pca)  # Use the model to make predictions\n",
    "\n",
    "performance = pd.DataFrame({\"Actual\" : np.squeeze(y_test),\n",
    "                            \"Predicted\" : y_hat})\n",
    "performance.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate acc\n",
    "acc_train = accuracy_score(y_train, model.predict(X_train_pca))\n",
    "acc_val = accuracy_score(y_test, model.predict(X_test_pca))\n",
    "\n",
    "print(f\"acc_train = {acc_train:.2f}\")\n",
    "print(f\"acc_val = {acc_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = cross_val_multiscore(model, X_train, y_train, cv=20, n_jobs=None)\n",
    "# # Mean scores across cross-validation splits\n",
    "# score = np.mean(scores, axis=0)\n",
    "# print(f\"Spatio-temporal: {100 * score:0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test.ravel(), y_pred)\n",
    "\n",
    "print(\"Accuracy score: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Get the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Unique labels in test set:\", np.unique(y_test))\n",
    "\n",
    "print(\"Unique predicted labels:\", np.unique(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# # save the model to disk\n",
    "# filename = '/home/Duchuy220602/thesis/Model/bag.sav'\n",
    "# joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Load the model from the .sav file\n",
    "# # import joblib\n",
    "\n",
    "# # model = joblib.load('/home/Duchuy220602/thesis/Model/pick.pkl')\n",
    "\n",
    "# # Load the model from the .pkl file\n",
    "# with open('/home/Duchuy220602/thesis/Model/pick.pkl', 'rb') as file:\n",
    "#     loaded_model = pickle.load(file)\n",
    "\n",
    "# # Step 2: Preprocess the CSV file data (if necessary)\n",
    "# import pandas as pd\n",
    "\n",
    "# data = pd.read_csv('/home/Duchuy220602/thesis/file_feature/plm.csv')\n",
    "\n",
    "\n",
    "# # Preprocess the data using a standard scaler\n",
    "# scaler = StandardScaler()\n",
    "# data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# # Perform any required preprocessing here, e.g., feature scaling, one-hot encoding, etc.\n",
    "# # For this example, I assume there is no preprocessing needed.\n",
    "\n",
    "# # Make predictions using the loaded model\n",
    "# predicted = loaded_model.predict(data_scaled)\n",
    "\n",
    "# # Step 3: Make predictions using the model\n",
    "# print(\"Predicted:\", predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from collections import Counter\n",
    "# import joblib\n",
    "\n",
    "# # Load the model from the .sav file\n",
    "# model_path = os.path.join(os.path.expanduser('~'), 'thesis', 'Model', 'SVM.sav')\n",
    "# loaded_model = joblib.load(model_path)\n",
    "\n",
    "# # # Load the model from the .pkl file\n",
    "# # model_path = os.path.join(os.path.expanduser('~'), 'thesis', 'Model', 'pick.pkl')\n",
    "# # with open(model_path, 'rb') as file:\n",
    "# #     loaded_model = pickle.load(file)\n",
    "\n",
    "# # Load the CSV file data\n",
    "# data_path = os.path.join(os.path.expanduser('~'), 'thesis', 'file_feature', 'ins.csv')\n",
    "# features = pd.read_csv(data_path)\n",
    "\n",
    "# # Preprocess the data using a standard scaler\n",
    "# scaler = StandardScaler()  # or StandardScaler() if you prefer\n",
    "# features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# # Apply PCA to scaled features\n",
    "# n_components = 60  # Adjust this number as needed\n",
    "# pca = PCA(n_components=n_components)\n",
    "# features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# # Make predictions using the loaded model and PCA-transformed features\n",
    "# predicted_pca = loaded_model.predict(features_pca)\n",
    "\n",
    "# # Calculate the percentage of each predicted class\n",
    "# class_counts_pca = Counter(predicted_pca)\n",
    "\n",
    "# total_samples_pca = len(predicted_pca)\n",
    "# for class_label, count in class_counts_pca.items():\n",
    "#     percentage_pca = (count / total_samples_pca) * 100\n",
    "#     print(f\"Class {class_label}: {percentage_pca:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Calculate the percentage of each predicted class\n",
    "# class_counts = Counter(percentage_pca)\n",
    "# total_samples = len(class_label)\n",
    "# class_percentages = [(count / total_samples) * 100 for count in class_counts.values()]\n",
    "\n",
    "# # Plot the percentage of each predicted class as a pie chart\n",
    "# plt.pie(class_percentages, labels=class_counts.keys(), autopct='%1.1f%%')\n",
    "# plt.axis('equal')\n",
    "# plt.title('Percentage of Each Predicted Class')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
